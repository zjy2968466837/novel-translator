# -*- coding: utf-8 -*-
"""
novel_translator.engine - ç¿»è¯‘å¼•æ“æ ¸å¿ƒæ¨¡å—

åŠŸèƒ½:
- æ”¯æŒ Chat / Completion åŒæ¨¡å‹åç«¯ (è‡ªåŠ¨æ£€æµ‹)
- æ”¯æŒå¹¶å‘ç¿»è¯‘åŠ é€Ÿ
- æ”¯æŒæ–­ç‚¹ç»­ä¼  (checkpoint)
- æ”¯æŒ TXT / EPUB è¾“å‡ºæ ¼å¼
- å‰æ–‡ä¸Šä¸‹æ–‡æ³¨å…¥ï¼Œä¿æŒè¯‘åä¸€è‡´
- æ•´ç« ç¿»è¯‘æ¨¡å¼ (chunk_size=0)
- è´¨é‡æ‰«æä¸é€‰æ‹©æ€§é‡ç¿»
"""

import os
import re
import time
import json
import hashlib
import threading
import warnings

import ebooklib
import ebooklib.utils as _ebooklib_utils
from ebooklib import epub
from bs4 import BeautifulSoup

# â”€â”€ Monkey-patch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ä¿®å¤ ebooklib åœ¨ write_epub æ—¶å›  EpubNav å†…å®¹ä¸ºç©ºå¯¼è‡´ lxml è§£æå´©æºƒ
_original_get_pages = _ebooklib_utils.get_pages


def _safe_get_pages(item):
    try:
        body = item.get_body_content()
        if not body or not body.strip():
            return []
        return _original_get_pages(item)
    except Exception:
        return []


_ebooklib_utils.get_pages = _safe_get_pages
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from typing import Callable, Optional

from novel_translator.providers import create_provider, AIProvider


# =====================================================================
# æ•°æ®ç±»
# =====================================================================

@dataclass
class TranslationConfig:
    """ç¿»è¯‘ä»»åŠ¡é…ç½®"""

    # API
    provider: str = "openai"  # "openai" / "anthropic" / "google" / "ollama"
    api_key: str = ""
    base_url: str = "https://api.siliconflow.cn/v1"
    model_name: str = "deepseek-ai/DeepSeek-V3.2"
    model_type: str = "auto"  # "auto" / "chat" / "completion"

    # ç”Ÿæˆå‚æ•°
    temperature: float = 0.7
    top_p: float = 0.9
    frequency_penalty: float = 0.1
    presence_penalty: float = 0.0
    max_tokens: int = 8192

    # åˆ†å—ä¸å¹¶å‘
    chunk_size: int = 1500      # 0 = æ•´ç« ç¿»è¯‘
    concurrent_workers: int = 1
    retry_count: int = 3

    # æ–‡ä»¶
    input_file: str = ""
    output_file: str = "novel_translated.txt"
    output_format: str = "txt"  # "txt" / "epub"
    glossary_file: str = ""

    # ç« èŠ‚èŒƒå›´
    start_chapter: int = 0
    end_chapter: int = 0

    # æç¤ºè¯
    custom_prompt: str = ""

    # æ–­ç‚¹ç»­ä¼ 
    enable_checkpoint: bool = True

    # ä¸Šä¸‹æ–‡æ³¨å…¥
    context_lines: int = 5     # å‰æ–‡ä¸Šä¸‹æ–‡è¡Œæ•° (0=å…³é—­)

    # è¡¥å…¨æ¨¡å‹ä¸“ç”¨
    few_shot_examples: str = ""


@dataclass
class TranslationProgress:
    """è¿è¡Œæ—¶ç¿»è¯‘è¿›åº¦"""

    total_chapters: int = 0
    current_chapter: int = 0
    current_chapter_name: str = ""
    total_chunks: int = 0
    current_chunk: int = 0
    is_running: bool = False
    is_paused: bool = False
    is_cancelled: bool = False
    translated_chars: int = 0
    start_time: float = 0
    elapsed_time: float = 0


# =====================================================================
# è¾…åŠ©ç±»
# =====================================================================

class ChapterInfo:
    """EPUB ç« èŠ‚å…ƒæ•°æ®"""

    def __init__(self, index: int, name: str, content: str, item=None, html_content: str = ""):
        self.index = index
        self.name = name
        self.content = content        # çº¯æ–‡æœ¬ï¼ˆç”¨äºåˆ†å—å’Œç¿»è¯‘ï¼‰
        self.html_content = html_content  # åŸå§‹ HTMLï¼ˆç”¨äºç»“æ„ä¿ç•™è¾“å‡ºï¼‰
        self.char_count = len(content)
        self.item = item


class CheckpointManager:
    """æ–­ç‚¹ç»­ä¼ ç®¡ç†å™¨ â€” åŸºäº JSON æ–‡ä»¶"""

    def __init__(self, input_file: str, output_file: str):
        h = hashlib.md5(input_file.encode()).hexdigest()[:8]
        base = os.path.splitext(output_file)[0]
        self.checkpoint_file = f"{base}.checkpoint.json"
        self.data: dict = {"completed_chapters": {}, "config_hash": h}

    def load(self):
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, "r", encoding="utf-8") as f:
                    self.data = json.load(f)
            except Exception:
                self.data = {"completed_chapters": {}}
        return self.data

    def save(self):
        try:
            with open(self.checkpoint_file, "w", encoding="utf-8") as f:
                json.dump(self.data, f, ensure_ascii=False, indent=2)
        except Exception:
            pass

    def is_chapter_done(self, chapter_name: str) -> bool:
        return chapter_name in self.data.get("completed_chapters", {})

    def get_chapter_result(self, chapter_name: str) -> str:
        return self.data.get("completed_chapters", {}).get(chapter_name, "")

    def mark_chapter_done(self, chapter_name: str, translated_text: str):
        if "completed_chapters" not in self.data:
            self.data["completed_chapters"] = {}
        self.data["completed_chapters"][chapter_name] = translated_text
        self.save()

    def get_completed_count(self) -> int:
        return len(self.data.get("completed_chapters", {}))

    def clear(self):
        self.data = {"completed_chapters": {}}
        if os.path.exists(self.checkpoint_file):
            os.remove(self.checkpoint_file)


# =====================================================================
# ç¿»è¯‘å¼•æ“
# =====================================================================

class TranslatorEngine:
    """ç¿»è¯‘å¼•æ“æ ¸å¿ƒ â€” é©±åŠ¨ CLI ä¸ GUI"""

    def __init__(self, config: TranslationConfig):
        self.config = config
        self.progress = TranslationProgress()
        self.provider: Optional[AIProvider] = None
        self.glossary: dict = {}
        self.system_prompt: str = ""
        self._lock = threading.Lock()
        self._pause_event = threading.Event()
        self._pause_event.set()
        self.checkpoint: Optional[CheckpointManager] = None

        # å›è°ƒæ¥å£
        self.on_progress: Optional[Callable] = None
        self.on_log: Optional[Callable] = None
        self.on_error: Optional[Callable] = None
        self.on_complete: Optional[Callable] = None
        self.on_chapter_start: Optional[Callable] = None

    # â”€â”€ æ—¥å¿— â”€â”€

    def log(self, message: str):
        if self.on_log:
            self.on_log(message)

    # â”€â”€ Provider åˆå§‹åŒ– â”€â”€

    def _init_provider(self):
        """æ ¹æ® config.provider åˆ›å»ºå¯¹åº”çš„ AI Provider å®ä¾‹"""
        provider_type = self.config.provider or "openai"
        if not self.config.api_key and provider_type != "ollama":
            raise ValueError("è¯·å¡«å†™ API Key")
        self.provider = create_provider(
            provider_type=provider_type,
            api_key=self.config.api_key,
            base_url=self.config.base_url,
            model_name=self.config.model_name,
            model_type=self.config.model_type,
            temperature=self.config.temperature,
            top_p=self.config.top_p,
            frequency_penalty=self.config.frequency_penalty,
            presence_penalty=self.config.presence_penalty,
            max_tokens=self.config.max_tokens,
            few_shot_examples=self.config.few_shot_examples,
        )
        self.log(f"âœ… {self.provider.provider_name} å·²åˆå§‹åŒ– ({self.config.model_name})")

    # â”€â”€ æœ¯è¯­è¡¨ â”€â”€

    def load_glossary(self, filepath: str = "") -> dict:
        path = filepath or self.config.glossary_file
        if not path or not os.path.exists(path):
            self.log("â„¹ï¸ æœªåŠ è½½æœ¯è¯­è¡¨")
            return {}
        try:
            with open(path, "r", encoding="utf-8") as f:
                glossary = json.load(f)
            self.log(f"âœ… æœ¯è¯­è¡¨å·²åŠ è½½: {len(glossary)} æ¡")
            return glossary
        except Exception as e:
            self.log(f"âš ï¸ æœ¯è¯­è¡¨åŠ è½½å¤±è´¥: {e}")
            return {}

    def save_glossary(self, glossary: dict, filepath: str):
        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(glossary, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.log(f"âš ï¸ æœ¯è¯­è¡¨ä¿å­˜å¤±è´¥: {e}")

    # â”€â”€ æç¤ºè¯æ„å»º â”€â”€

    def build_system_prompt(self, glossary_dict: dict | None = None) -> str:
        if self.config.custom_prompt:
            base_prompt = self.config.custom_prompt
        else:
            base_prompt = (
                "ä½ æ˜¯ä¸€ä½ç²¾é€šä¸­æ—¥æ–‡åŒ–çš„ä¸“ä¸šè½»å°è¯´ç¿»è¯‘ä¸“å®¶ã€‚"
                "è¯·å°†ç”¨æˆ·è¾“å…¥çš„æ—¥æ–‡å¼‚ä¸–ç•Œè½¬ç”Ÿå°è¯´ç‰‡æ®µç¿»è¯‘æˆæµç•…ã€åœ°é“çš„ä¸­æ–‡ã€‚\n\n"
                "æ ¸å¿ƒç¿»è¯‘åŸåˆ™ï¼š\n"
                "1. ä¸¥æ ¼å¿ å®åŸæ–‡ï¼šå‡†ç¡®ä¼ è¾¾åŸæ–‡å«ä¹‰ï¼Œä¸å¢åŠ ã€ä¸åˆ å‡ã€ä¸æ”¹å†™ä»»ä½•å†…å®¹ã€‚åŸæ–‡æ²¡æœ‰çš„è¯­æ°”ã€æƒ…ç»ªã€è¯­æ°”è¯ç»å¯¹ä¸èƒ½æ·»åŠ ã€‚\n"
                "2. ç¦æ­¢æ·»åŠ è¯­æ°”è¯ï¼šä¸å¾—è‡ªè¡Œæ·»åŠ åŸæ–‡ä¸­ä¸å­˜åœ¨çš„\u201cå‘€\u201d\u201cå‘¢\u201d\u201cå˜›\u201d\u201cå“¦\u201d\u201cå•¦\u201d\u201cå“Ÿ\u201d\u201cå‘ƒ\u201dç­‰è¯­æ°”è¯ã€‚"
                "åªæœ‰åŸæ–‡æ˜ç¡®åŒ…å«å¯¹åº”çš„æ—¥æ–‡è¯­æ°”è¯ï¼ˆå¦‚ã€Œã­ã€ã€Œã‚ˆã€ã€Œã•ã€ã€Œãã€ã€Œãªã€ç­‰ï¼‰æ—¶ï¼Œæ‰å¯ä»¥ç¿»è¯‘ä¸ºç›¸åº”çš„ä¸­æ–‡è¯­æ°”è¯ã€‚\n"
                "3. å…‹åˆ¶\u201cå§\u201dçš„ä½¿ç”¨ï¼š\u201cå§\u201dåªåœ¨åŸæ–‡æ˜ç¡®è¡¨è¾¾æ¨æµ‹ã€å»ºè®®ã€è¯·æ±‚è¯­æ°”æ—¶ä½¿ç”¨ï¼Œé™ˆè¿°å¥ä¸­ä¸å¾—æ»¥ç”¨ã€‚\n"
                "4. æœ¬åœŸåŒ–è¡¨è¾¾ï¼šä½¿ç”¨ç®€æ´ã€ç¬¦åˆä¸­æ–‡ä¹¦é¢è¯­ä¹ æƒ¯çš„è‡ªç„¶è¯­å¥ï¼Œé¿å…æ—¥å¼ç›´è¯‘å’Œæœºç¿»è…”è°ƒã€‚\n"
                "5. å¼‚ä¸–ç•Œæ°›å›´ï¼šå®Œæ•´ä¿ç•™ä¸“æœ‰åè¯ã€é­”æ³•ä½“ç³»ã€ç­‰çº§åˆ¶åº¦ç­‰ä¸–ç•Œè§‚å…ƒç´ ã€‚\n"
                "6. è§’è‰²è¯­æ°”ï¼šä¿ç•™åŸæ–‡è§’è‰²çš„è¯´è¯é£æ ¼ï¼Œä½†ä¸è¦è¿‡åº¦æ¼”ç»æˆ–å¤¸å¼ åŒ–ã€‚\n"
                "7. æ®µè½ä¸æ–­å¥ï¼šå¯¹è¯ä½¿ç”¨ã€Œã€æˆ–\u201c\u201dã€‚åŸæ–‡ä¸­è¯­æ„è¿è´¯çš„ç›¸é‚»çŸ­å¥åº”åˆå¹¶ä¸ºæµç•…çš„é•¿å¥ï¼Œä¸è¦é€å¥æœºæ¢°æ–­è¡Œï¼›"
                "ä»…åœ¨è¯é¢˜è½¬æ¢ã€åœºæ™¯åˆ‡æ¢æˆ–åŸæ–‡æ˜ç¡®åˆ†æ®µå¤„å¦èµ·æ–°æ®µã€‚\n"
                "8. æœ¯è¯­ç»Ÿä¸€ï¼šä¸¥æ ¼éµå®ˆæœ¯è¯­è¡¨ä¸­çš„è¯‘åã€‚\n"
                "9. è¯­ä½“é€‚é…ï¼šç¬¬ä¸€äººç§°å†…å¿ƒç‹¬ç™½å’Œæ—¥å¸¸å¯¹è¯ä½¿ç”¨ç°ä»£å£è¯­ä½“ï¼Œç¦ç”¨æ–‡è¨€æˆ–è¿‡åº¦ä¹¦é¢åŒ–æªè¾"
                "ï¼ˆå¦‚\u201cä½•ä»¥è§å¾—\u201d\u201cæœ‰ä½•è´µå¹²\u201d\u201cæ„¿é—»å…¶è¯¦\u201dç­‰ï¼‰ã€‚ä»…åœ¨åŸæ–‡ä½¿ç”¨æ­£å¼/å¤é£è¯­ä½“çš„è§’è‰²å°è¯ä¸­æ–¹å¯ä½¿ç”¨å¯¹åº”æ–‡ä½“ã€‚\n"
                "10. æ—¶æ€å‡†ç¡®ï¼šé˜è¿°ä¸–ç•Œè§‚è®¾å®šå’Œä¸€èˆ¬æ€§è§„åˆ™æ—¶ä½¿ç”¨ä¸€èˆ¬æ—¶æ€ï¼Œä¸è¦è¯¯ç”¨å®Œæˆæ—¶æ€\u201cäº†\u201dã€‚å™è¿°å·²å‘ç”Ÿäº‹ä»¶æ—¶æ­£å¸¸ä½¿ç”¨ã€‚\n"
                "11. ç¦æ­¢æ·»è¯ï¼šä¸å¾—æ·»åŠ åŸæ–‡ä¸­æ²¡æœ‰çš„åè¯ã€é‡è¯æˆ–ä¿®é¥°è¯­ã€‚æ—¥æ–‡æ‹Ÿæ€è¯ï¼ˆå¦‚ãƒŒãƒ©ãƒŒãƒ©ã€ãƒãƒƒãƒˆãƒªç­‰ï¼‰"
                "åº”è¯‘ä¸ºå¯¹åº”æ„Ÿè§‰çš„ä¸­æ–‡è¡¨è¾¾ï¼Œä¸å¯æ“…è‡ªè¡¥å……å…·ä½“åè¯ã€‚\n"
                "12. çº¯å‡€è¾“å‡ºï¼šåªè¾“å‡ºç¿»è¯‘æ­£æ–‡ï¼Œä¸¥ç¦è¾“å‡ºä»»ä½•ç¿»è¯‘æ³¨é‡Šã€è¯‘è€…æ³¨ã€è„šæ³¨ã€è¯´æ˜æ–‡å­—ã€æ‹¬å·è¡¥å……è§£é‡Šã€‚"
                "ä¸å¾—æ·»åŠ \u201cæ³¨ï¼š\u201dã€\u201cè¯‘æ³¨ï¼š\u201dã€\u201c*\u201dæ³¨é‡Šã€ä»»ä½•metaå†…å®¹ã€‚\n"
                "13. æœ¯è¯­å‰åä¸€è‡´ï¼šåŒä¸€ä¸“æœ‰åè¯åœ¨å…¨æ–‡ä¸­å¿…é¡»ä½¿ç”¨å®Œå…¨ç›¸åŒçš„è¯‘åå’Œæ ‡è®°æ ¼å¼ã€‚"
                "ä¾‹å¦‚ï¼šã€é‡‘å‰›ã€å§‹ç»ˆè¯‘ä¸ºã€Œé‡‘åˆšã€ã€ç”Ÿæ¶¯ã®é­”æ³•å§‹ç»ˆè¯‘ä¸º\u201cç»ˆç”Ÿé­”æ³•\u201dã€ã‚¦ãƒ«ã‚¿ã‚¹å§‹ç»ˆè¯‘ä¸º\u201cå„å°”å¡”æ–¯\u201dã€"
                "ãƒãƒŠå§‹ç»ˆè¯‘ä¸º\u201cé­”åŠ›ç´ \u201dæˆ–æœ¯è¯­è¡¨æŒ‡å®šè¯‘åã€‚ç¦æ­¢åœ¨ä¸åŒæ®µè½ä¸­å¯¹åŒä¸€æœ¯è¯­ä½¿ç”¨ä¸åŒè¯‘æ³•ã€‚\n"
                "14. æ ‡è®°ç»Ÿä¸€ï¼šä¸“æœ‰åè¯ä¸€å¾‹ä½¿ç”¨ã€Œã€æ ‡è®°ï¼ˆå¦‚ã€Œé‡‘åˆšã€ã€Œé­…æƒ‘ä¹‹ç³ã€ï¼‰ï¼Œ"
                "ä¸å¾—æ··ç”¨ã€ã€ã€ã€Šã€‹ã€ã€ã€‘ã€\u201c\u201dç­‰ä¸åŒæ ‡è®°ç¬¦å·ã€‚\n"
                "15. ç§°å‘¼ç¿»è¯‘ï¼šæ—¥æ–‡\u201cå…ˆè¼©\u201dåœ¨å­¦å›­èƒŒæ™¯ä¸‹ï¼Œå¿…é¡»æ ¹æ®æ€§åˆ«ç¿»è¯‘â€”â€”"
                "å¥³æ€§å…ˆè¼©ä¸€å¾‹è¯‘ä¸º\u201cå­¦å§\u201dï¼Œç”·æ€§å…ˆè¼©ä¸€å¾‹è¯‘ä¸º\u201cå­¦é•¿\u201dã€‚"
                "ä¸¥ç¦ä½¿ç”¨\u201cå‰è¾ˆ\u201dè¿™ä¸€æ€§åˆ«æ¨¡ç³Šçš„è¯‘æ³•ã€‚åŒä¸€è§’è‰²çš„ç§°å‘¼åœ¨å…¨æ–‡ä¸­å¿…é¡»ä¿æŒå®Œå…¨ä¸€è‡´ï¼Œä¸å¾—åœ¨ä¸åŒæ®µè½é—´åˆ‡æ¢ç”¨è¯ã€‚\n"
                "16. äººåä¸€è‡´æ€§ï¼šåŒä¸€è§’è‰²åœ¨å…¨æ–‡ä¸­å¿…é¡»ä½¿ç”¨å®Œå…¨ç›¸åŒçš„ä¸­æ–‡è¯‘åï¼Œä¸¥ç¦å‡ºç°å˜ä½“ã€‚"
                "ä¾‹å¦‚ï¼šãƒŸãƒ¤å§‹ç»ˆè¯‘ä¸º\u201cå¼¥å¨…\u201dï¼ˆä¸å¯å‡ºç°\u201cç±³å¨…\u201d\u201cç±³äºš\u201d\u201cå®«\u201dç­‰å˜ä½“ï¼‰ï¼›"
                "ã‚¯ãƒªã‚¹å§‹ç»ˆè¯‘ä¸º\u201cå…‹é‡Œæ–¯\u201dï¼ˆä¸å¯å‡ºç°\u201cå…‹è‰ä¸\u201dç­‰å˜ä½“ï¼‰ï¼›"
                "ã‚°ãƒªãƒ¼ã‚¸ãƒ£ãƒ¼çš„ä¸­æ–‡åå§‹ç»ˆä¸º\u201cå®‰æ¶…è‰ä¸\u201dï¼ˆä¸å¯å‡ºç°\u201cæ ¼é‡Œæ°å°”\u201d\u201cæ ¼é‡Œæ°\u201dç­‰éŸ³è¯‘å˜ä½“ï¼‰ã€‚"
                "å½“åŸæ–‡å‡ºç°å…¨åæ—¶ï¼ˆå¦‚ã‚¢ãƒã‚¹ãƒˆãƒ»ã‚°ãƒªãƒ¼ã‚¸ãƒ£ãƒ¼ï¼‰ï¼Œè¯‘ä¸º\u201cå®‰æ¶…è‰ä¸Â·æ ¼é‡Œæ°å°”\u201dã€‚\n\n"
                "ç¿»è¯‘é£æ ¼ï¼šç®€æ´å‡†ç¡®ï¼Œç´§è´´åŸæ–‡ï¼Œè¯­æ„è¿è´¯çš„çŸ­å¥åˆå¹¶ä¸ºæµç•…é•¿å¥ï¼Œä¸æ·»åŠ åŸæ–‡æ²¡æœ‰çš„ä¿®è¾å’Œè¯­æ°”ã€‚\n"
            )
        g = glossary_dict if glossary_dict is not None else self.glossary
        if g:
            glossary_text = "\nã€å¼ºåˆ¶æœ¯è¯­è¡¨ã€‘\n"
            for k, v in g.items():
                glossary_text += f"- {k} -> {v}\n"
            return base_prompt + glossary_text
        return base_prompt

    def build_completion_prompt(self, text: str, prev_context: str = "") -> str:
        """ä¸ºè¡¥å…¨æ¨¡å‹æ„å»ºå®Œæ•´ promptï¼ˆå« few-shot ç¤ºä¾‹ + æœ¯è¯­è¡¨ + ä¸Šä¸‹æ–‡ + åŸæ–‡ï¼‰"""
        parts = []
        parts.append("ä»¥ä¸‹æ˜¯æ—¥æ–‡è½»å°è¯´ç¿»è¯‘ä»»åŠ¡ã€‚è¯·å°†ã€å¾…ç¿»è¯‘åŸæ–‡ã€‘ç¿»è¯‘ä¸ºæµç•…çš„ä¸­æ–‡ï¼Œåªè¾“å‡ºè¯‘æ–‡ã€‚\n")

        g = self.glossary
        if g:
            parts.append("ã€æœ¯è¯­è¡¨ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ã€‘")
            for k, v in g.items():
                parts.append(f"- {k} â†’ {v}")
            parts.append("")

        if self.config.few_shot_examples:
            parts.append(self.config.few_shot_examples)
            parts.append("")

        if prev_context:
            parts.append("ã€å‰æ–‡è¯‘æ–‡å‚è€ƒï¼ˆä¿æŒäººåã€ç§°è°“ä¸€è‡´ï¼‰ã€‘")
            parts.append(prev_context)
            parts.append("")

        parts.append("ã€å¾…ç¿»è¯‘åŸæ–‡ã€‘")
        parts.append(text)
        parts.append("")
        parts.append("ã€è¯‘æ–‡ã€‘")

        return "\n".join(parts)

    # â”€â”€ æ–‡æœ¬å¤„ç† â”€â”€

    # éœ€ä¿ç•™çš„è¡Œå†…æ ‡ç­¾ï¼ˆç¿»è¯‘å†…éƒ¨æ–‡æœ¬ä½†ä¿ç•™æ ‡ç­¾ç»“æ„ï¼‰
    _INLINE_TAGS = {'em', 'strong', 'b', 'i', 'u', 's', 'span', 'a', 'small', 'sub', 'sup', 'mark'}
    # Ruby æ³¨éŸ³æ ‡ç­¾ï¼ˆä¿ç•™åŸæ ·ä¸ç¿»è¯‘ï¼‰
    _RUBY_TAGS = {'ruby', 'rt', 'rp', 'rb'}
    # å—çº§å…ƒç´ ï¼ˆæ¯ä¸ªäº§ç”Ÿä¸€ä¸ªç¿»è¯‘æ®µè½ï¼‰
    _BLOCK_TAGS = {'p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'blockquote', 'li', 'dt', 'dd', 'figcaption'}
    # ä¸ç¿»è¯‘çš„æ ‡ç­¾ï¼ˆä¿ç•™åŸæ ·ï¼‰
    _SKIP_TAGS = {'img', 'image', 'svg', 'br', 'hr', 'table', 'thead', 'tbody', 'tr', 'td', 'th', 'script', 'style'}

    @staticmethod
    def clean_html(html_content) -> str:
        """å°† HTML è½¬æ¢ä¸ºçº¯æ–‡æœ¬ï¼ˆå‘åå…¼å®¹ï¼‰"""
        warnings.filterwarnings("ignore", category=UserWarning, module="bs4")
        soup = BeautifulSoup(html_content, "html.parser")
        return soup.get_text(separator="\n", strip=True)

    @staticmethod
    def parse_html_structured(html_content) -> tuple[str, list[dict]]:
        """ç»“æ„æ„ŸçŸ¥çš„ HTML è§£æã€‚

        è¿”å›:
            (plain_text, segments)
            - plain_text: ç”¨äºåˆ†å—å’Œç¿»è¯‘çš„çº¯æ–‡æœ¬
            - segments: æ¯ä¸ªå…ƒç´ çš„ç»“æ„ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å«:
              - type: "text" | "image" | "heading" | "skip"
              - tag: åŸå§‹æ ‡ç­¾å
              - text: æå–çš„çº¯æ–‡æœ¬
              - html: åŸå§‹ HTML ç‰‡æ®µ
              - attrs: æ ‡ç­¾å±æ€§å­—å…¸
        """
        warnings.filterwarnings("ignore", category=UserWarning, module="bs4")
        soup = BeautifulSoup(html_content, "html.parser")
        body = soup.find("body")
        if not body:
            body = soup

        segments = []
        text_parts = []

        for element in body.children:
            if isinstance(element, str):
                # è£¸æ–‡æœ¬èŠ‚ç‚¹
                stripped = element.strip()
                if stripped:
                    segments.append({"type": "text", "tag": "", "text": stripped, "html": stripped, "attrs": {}})
                    text_parts.append(stripped)
                continue

            tag_name = getattr(element, 'name', None)
            if not tag_name:
                continue

            if tag_name in TranslatorEngine._SKIP_TAGS:
                # å›¾ç‰‡ã€è¡¨æ ¼ç­‰ä¸ç¿»è¯‘ï¼ŒåŸæ ·ä¿ç•™
                seg_type = "image" if tag_name in ('img', 'image', 'svg') else "skip"
                segments.append({
                    "type": seg_type, "tag": tag_name,
                    "text": "", "html": str(element), "attrs": dict(element.attrs) if hasattr(element, 'attrs') else {},
                })
                continue

            if tag_name in TranslatorEngine._BLOCK_TAGS or tag_name.startswith('h'):
                # å—çº§å…ƒç´ â€”â€”æå–æ–‡æœ¬ç”¨äºç¿»è¯‘ï¼Œä¿ç•™å†…è”æ ‡ç­¾ç»“æ„
                inner_text = element.get_text(strip=True)
                if not inner_text:
                    # ç©ºå—çº§å…ƒç´ ï¼ˆå¯èƒ½å«å›¾ç‰‡ï¼‰ï¼Œä¿ç•™åŸæ ·
                    segments.append({"type": "skip", "tag": tag_name, "text": "", "html": str(element), "attrs": {}})
                    continue
                seg_type = "heading" if tag_name.startswith('h') else "text"
                segments.append({
                    "type": seg_type, "tag": tag_name,
                    "text": inner_text, "html": str(element),
                    "attrs": dict(element.attrs) if hasattr(element, 'attrs') else {},
                })
                text_parts.append(inner_text)
                continue

            # å…¶ä»–å…ƒç´ ï¼ˆå¦‚ section, article, div åµŒå¥—ï¼‰â€”â€”é€’å½’æå–
            inner_text = element.get_text(separator="\n", strip=True)
            if inner_text:
                segments.append({"type": "text", "tag": tag_name, "text": inner_text, "html": str(element), "attrs": {}})
                text_parts.append(inner_text)

        plain_text = "\n".join(text_parts)
        return plain_text, segments

    @staticmethod
    def rebuild_chapter_html(segments: list[dict], translated_text: str, original_html: str = "") -> str:
        """å°†ç¿»è¯‘ç»“æœå›æ³¨åˆ°åŸå§‹ HTML ç»“æ„ä¸­ã€‚

        ç­–ç•¥ï¼šæŒ‰æ®µè½é¡ºåºå°†ç¿»è¯‘æ–‡æœ¬å¡«å›å¯¹åº”çš„ segmentï¼Œ
        ä¿ç•™éæ–‡æœ¬ segmentï¼ˆå›¾ç‰‡ã€è¡¨æ ¼ç­‰ï¼‰åŸæ ·ä¸åŠ¨ã€‚
        """
        trans_paragraphs = [p.strip() for p in translated_text.split("\n") if p.strip()]
        trans_idx = 0
        result_parts = []

        for seg in segments:
            if seg["type"] in ("image", "skip"):
                # éæ–‡æœ¬å…ƒç´ åŸæ ·ä¿ç•™
                result_parts.append(seg["html"])
            elif seg["type"] in ("text", "heading"):
                tag = seg.get("tag", "p") or "p"
                if trans_idx < len(trans_paragraphs):
                    trans_content = trans_paragraphs[trans_idx]
                    # HTML è½¬ä¹‰
                    trans_content = trans_content.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
                    result_parts.append(f"<{tag}>{trans_content}</{tag}>")
                    trans_idx += 1
                else:
                    # ç¿»è¯‘æ®µè½ä¸è¶³ï¼Œä¿ç•™åŸæ–‡
                    result_parts.append(seg["html"])
            else:
                result_parts.append(seg["html"])

        # å¦‚æœç¿»è¯‘æ®µè½æ¯” segment å¤šï¼ˆæ¨¡å‹æ‹†åˆ†äº†æ®µè½ï¼‰ï¼Œè¿½åŠ å‰©ä½™éƒ¨åˆ†
        while trans_idx < len(trans_paragraphs):
            extra = trans_paragraphs[trans_idx].replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
            result_parts.append(f"<p>{extra}</p>")
            trans_idx += 1

        return "\n".join(result_parts)

    @staticmethod
    def _extract_chapter_order_key(filename: str):
        """ä»æ–‡ä»¶åä¸­æå–æ’åºé”®"""
        basename = os.path.basename(filename).lower()
        if basename in ("nav.xhtml", "toc.xhtml", "cover.xhtml"):
            return (0, 0)
        m = re.search(r"(\d+)", basename)
        if m:
            return (1, int(m.group(1)))
        return (2, 0)

    @staticmethod
    def _sort_chapters_data(chapters_data: list) -> list:
        return sorted(
            chapters_data,
            key=lambda x: TranslatorEngine._extract_chapter_order_key(x[0]),
        )

    @staticmethod
    def _extract_chapter_title(content: str, fallback_index=None):
        """ä»ç¿»è¯‘å†…å®¹é¦–è¡Œæå–ç« èŠ‚æ ‡é¢˜ï¼Œè¿”å› (æ ‡é¢˜, æ­£æ–‡)"""
        if not content or not content.strip():
            title = f"ç¬¬{fallback_index}ç« " if fallback_index is not None else "æœªå‘½åç« èŠ‚"
            return title, content or ""

        lines = content.strip().split("\n")
        first_line = lines[0].strip()

        if first_line and len(first_line) <= 30:
            title = first_line
            body = "\n".join(lines[1:]).strip()
            return title, body

        title = f"ç¬¬{fallback_index}ç« " if fallback_index is not None else first_line[:20]
        return title, content.strip()

    def split_text(self, text: str) -> list[str]:
        if not text:
            return []
        max_chars = self.config.chunk_size
        if max_chars <= 0:
            return [text.strip()]
        paragraphs = text.split("\n")
        chunks = []
        current_chunk = ""
        for p in paragraphs:
            p = p.strip()
            if not p:
                continue
            if len(current_chunk) + len(p) > max_chars and current_chunk:
                chunks.append(current_chunk)
                current_chunk = p + "\n"
            else:
                current_chunk += p + "\n"
        if current_chunk:
            chunks.append(current_chunk)
        return chunks

    # â”€â”€ ç¿»è¯‘æ ¸å¿ƒ â”€â”€

    def translate_chunk(self, text: str, prev_context: str = "") -> str:
        if not text.strip():
            return ""

        # æ„å»ºç”¨æˆ·å†…å®¹ï¼ˆå¸¦ä¸Šä¸‹æ–‡ï¼‰
        if prev_context:
            user_content = (
                f"[å‰æ–‡ç¿»è¯‘å‚è€ƒï¼ˆä»…ä¾›ä¿æŒäººåã€ç§°è°“ã€æœ¯è¯­ä¸€è‡´ï¼Œè¯·å‹¿ç¿»è¯‘æ­¤éƒ¨åˆ†ï¼‰]\n"
                f"{prev_context}\n\n"
                f"[å¾…ç¿»è¯‘åŸæ–‡]\n{text}"
            )
        else:
            user_content = text

        for attempt in range(self.config.retry_count):
            self._pause_event.wait()
            if self.progress.is_cancelled:
                return "[ç¿»è¯‘å·²å–æ¶ˆ]"
            try:
                result = self.provider.translate(self.system_prompt, user_content)
                return result
            except Exception as e:
                err_detail = self._format_api_error(e)
                self.log(f"âš ï¸ API è°ƒç”¨å¤±è´¥ (å°è¯• {attempt+1}/{self.config.retry_count}): {err_detail}")
                if attempt < self.config.retry_count - 1:
                    wait = 2 * (attempt + 1)
                    retry_after = self._get_retry_after(e)
                    if retry_after:
                        wait = max(wait, retry_after)
                        self.log(f"â³ æœåŠ¡ç«¯è¦æ±‚ç­‰å¾… {retry_after}s (retry-after)")
                    time.sleep(wait)
                else:
                    return f"\n[ç¿»è¯‘å¤±è´¥: {err_detail}]\n"
        return "[ç¿»è¯‘å¤±è´¥: æœªçŸ¥é”™è¯¯]"

    # â”€â”€ é”™è¯¯æ ¼å¼åŒ– â”€â”€

    @staticmethod
    def _format_api_error(e) -> str:
        parts = []
        status = getattr(e, "status_code", None)
        if status:
            status_map = {
                400: "è¯·æ±‚æ ¼å¼é”™è¯¯", 401: "è®¤è¯å¤±è´¥(Keyæ— æ•ˆ)", 402: "ä½™é¢ä¸è¶³",
                403: "æƒé™ä¸è¶³", 404: "æ¨¡å‹/ç«¯ç‚¹ä¸å­˜åœ¨", 429: "è¯·æ±‚é™é€Ÿ(è§¦å‘é€Ÿç‡é™åˆ¶)",
                500: "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯", 502: "ç½‘å…³é”™è¯¯", 503: "æœåŠ¡æš‚ä¸å¯ç”¨",
            }
            desc = status_map.get(status, "")
            parts.append(f"HTTP {status}" + (f" ({desc})" if desc else ""))

        body = getattr(e, "body", None)
        if isinstance(body, dict):
            err_msg = body.get("message", "") or body.get("error", {}).get("message", "")
            err_type = body.get("type", "") or body.get("error", {}).get("type", "")
            if err_type:
                parts.append(f"ç±»å‹={err_type}")
            if err_msg:
                parts.append(err_msg[:200])
        elif body:
            parts.append(str(body)[:200])

        response = getattr(e, "response", None)
        if response:
            headers = getattr(response, "headers", None)
            if headers:
                req_id = headers.get("x-request-id") or headers.get("X-Request-Id")
                if req_id:
                    parts.append(f"è¯·æ±‚ID={req_id}")

        if not parts:
            etype = type(e).__name__
            return f"[{etype}] {str(e)[:200]}"
        return " | ".join(parts)

    @staticmethod
    def _get_retry_after(e) -> int | None:
        response = getattr(e, "response", None)
        if response:
            headers = getattr(response, "headers", None)
            if headers:
                ra = headers.get("retry-after")
                if ra:
                    try:
                        return int(ra)
                    except (ValueError, TypeError):
                        pass
        return None

    # â”€â”€ ç« èŠ‚è¯»å– â”€â”€

    def get_chapters(self) -> list[ChapterInfo]:
        if not os.path.exists(self.config.input_file):
            raise FileNotFoundError(f"æœªæ‰¾åˆ°æ–‡ä»¶: {self.config.input_file}")
        book = epub.read_epub(self.config.input_file)
        try:
            items = list(book.get_items_of_type(ebooklib.ITEM_DOCUMENT))
        except (KeyError, AttributeError):
            items = [x for x in book.get_items() if x.get_type() == ebooklib.ITEM_DOCUMENT]
        chapters = []
        seen_names = set()
        for idx, item in enumerate(items):
            name = item.get_name()
            if name in seen_names:
                continue
            seen_names.add(name)
            raw_content = item.get_content()
            clean_text = self.clean_html(raw_content)
            if len(clean_text) >= 50:
                # åŒæ—¶å­˜å‚¨åŸå§‹ HTML ä»¥ä¾¿åç»­ç»“æ„ä¿ç•™
                html_str = raw_content.decode('utf-8', errors='replace') if isinstance(raw_content, bytes) else str(raw_content)
                chapters.append(ChapterInfo(idx + 1, name, clean_text, item, html_content=html_str))
        return chapters

    # â”€â”€ ä¸Šä¸‹æ–‡æ³¨å…¥ â”€â”€

    def _get_context_tail(self, text: str, n_lines: int | None = None) -> str:
        if n_lines is None:
            n_lines = self.config.context_lines
        if not text or n_lines <= 0:
            return ""
        lines = [l.strip() for l in text.strip().split("\n") if l.strip()]
        tail = lines[-n_lines:] if len(lines) > n_lines else lines
        return "\n".join(tail)

    # â”€â”€ åˆ†å—ç¿»è¯‘ â”€â”€

    def _translate_chunks(self, chunks: list[str]) -> list[str]:
        results = [None] * len(chunks)
        context_lines = self.config.context_lines

        def _do(index, chunk_text, prev_ctx=""):
            result = self.translate_chunk(chunk_text, prev_context=prev_ctx)
            with self._lock:
                self.progress.current_chunk += 1
                self.progress.translated_chars += len(result)
            if self.on_progress:
                self.on_progress(self.progress)
            return index, result

        workers = min(self.config.concurrent_workers, len(chunks))
        if workers <= 1:
            prev_ctx = ""
            for i, chunk in enumerate(chunks):
                if self.progress.is_cancelled:
                    break
                idx, result = _do(i, chunk, prev_ctx)
                results[idx] = result
                prev_ctx = self._get_context_tail(result, context_lines)
        else:
            if context_lines > 0:
                self.log("ğŸ’¡ å¹¶å‘æ¨¡å¼ä¸‹ä¸Šä¸‹æ–‡æ³¨å…¥ä»…åœ¨æ‰¹æ¬¡é—´ç”Ÿæ•ˆ")
            batch_prev_ctx = ""
            for batch_start in range(0, len(chunks), workers):
                batch_end = min(batch_start + workers, len(chunks))
                batch = list(enumerate(chunks[batch_start:batch_end], start=batch_start))
                with ThreadPoolExecutor(max_workers=workers) as executor:
                    futures = {}
                    for j, (i, c) in enumerate(batch):
                        ctx = batch_prev_ctx if (j == 0 and context_lines > 0) else ""
                        futures[executor.submit(_do, i, c, ctx)] = i
                    for future in as_completed(futures):
                        if self.progress.is_cancelled:
                            break
                        idx, result = future.result()
                        results[idx] = result
                last_result = results[batch_end - 1]
                if last_result:
                    batch_prev_ctx = self._get_context_tail(last_result, context_lines)

        return [r for r in results if r is not None]

    # â”€â”€ è¾“å‡ºå†™å…¥ â”€â”€

    def _write_txt(self, output_path: str, chapters_data: list):
        sorted_data = self._sort_chapters_data(chapters_data)
        with open(output_path, "w", encoding="utf-8") as f:
            for i, (filename, content) in enumerate(sorted_data):
                title, body = self._extract_chapter_title(content, fallback_index=i + 1)
                f.write(f"\n{'='*40}\n")
                f.write(f"  {title}\n")
                f.write(f"{'='*40}\n\n")
                f.write(body)
                f.write("\n\n")

    def _write_epub(self, output_path: str, chapters_data: list):
        """ç”Ÿæˆ EPUB è¾“å‡ºã€‚

        å¦‚æœæœ‰åŸå§‹ EPUB æºæ–‡ä»¶ï¼Œå°†å¤åˆ¶å…¶ CSS/å›¾ç‰‡/å­—ä½“/å…ƒæ•°æ®ï¼Œ
        å¹¶å°†ç¿»è¯‘ç»“æœæ³¨å…¥å¯¹åº”ç« èŠ‚çš„ HTML ä¸­ï¼Œä¿ç•™åŸå§‹æ ·å¼ã€‚
        å¦‚æœæ²¡æœ‰åŸå§‹æ–‡ä»¶ï¼Œå›é€€åˆ°ç®€å•æ„å»ºæ¨¡å¼ã€‚
        """
        sorted_data = self._sort_chapters_data(chapters_data)

        # å°è¯•ä»åŸå§‹ EPUB å¤åˆ¶èµ„æº
        source_book = None
        if self.config.input_file and os.path.exists(self.config.input_file):
            try:
                source_book = epub.read_epub(self.config.input_file)
            except Exception:
                pass

        book = epub.EpubBook()

        if source_book:
            # å¤åˆ¶å…ƒæ•°æ®
            for meta in source_book.metadata.get('http://purl.org/dc/elements/1.1/', []):
                # meta æ ¼å¼: (name, value, attrs)
                pass  # ebooklib çš„ metadata API è¾ƒå¤æ‚ï¼Œå…ˆè®¾ç½®åŸºæœ¬ä¿¡æ¯
            src_name = os.path.splitext(os.path.basename(self.config.input_file))[0]
            book.set_identifier("novel-translator-output")
            book.set_title(f"{src_name} (ä¸­æ–‡ç¿»è¯‘)")
            book.set_language("zh")
            book.add_author("AI Translation")

            # å¤åˆ¶æ‰€æœ‰éæ–‡æ¡£èµ„æºï¼ˆCSSã€å›¾ç‰‡ã€å­—ä½“ç­‰ï¼‰
            resource_items = []
            for item in source_book.get_items():
                item_type = item.get_type()
                if item_type == ebooklib.ITEM_DOCUMENT:
                    continue  # ç« èŠ‚æ–‡æ¡£å•ç‹¬å¤„ç†
                if item_type in (ebooklib.ITEM_STYLE, ebooklib.ITEM_IMAGE,
                                 ebooklib.ITEM_FONT, ebooklib.ITEM_COVER):
                    book.add_item(item)
                    resource_items.append(item)
                elif item_type not in (ebooklib.ITEM_NAVIGATION,):
                    # å…¶ä»–èµ„æºï¼ˆå¦‚åµŒå…¥å­—ä½“ã€éŸ³é¢‘ç­‰ï¼‰ä¹Ÿå¤åˆ¶
                    try:
                        book.add_item(item)
                    except Exception:
                        pass
            if resource_items:
                self.log(f"ğŸ“‚ å·²å¤åˆ¶ {len(resource_items)} ä¸ªåŸå§‹èµ„æºï¼ˆCSS/å›¾ç‰‡/å­—ä½“ï¼‰")
        else:
            book.set_identifier("novel-translator-output")
            src_name = os.path.splitext(os.path.basename(self.config.output_file))[0]
            book.set_title(f"{src_name}")
            book.set_language("zh")
            book.add_author("AI Translation")

        spine = ["nav"]
        toc = []

        # æ„å»ºç« èŠ‚ååˆ°ç¿»è¯‘å†…å®¹çš„æ˜ å°„
        translated_map = {name: content for name, content in sorted_data}

        # å¦‚æœæœ‰åŸå§‹ä¹¦ç±ï¼Œå°è¯•ä¿ç•™åŸå§‹ç« èŠ‚ç»“æ„
        if source_book:
            try:
                source_docs = list(source_book.get_items_of_type(ebooklib.ITEM_DOCUMENT))
            except (KeyError, AttributeError):
                source_docs = [x for x in source_book.get_items() if x.get_type() == ebooklib.ITEM_DOCUMENT]

            chapter_idx = 0
            for item in source_docs:
                name = item.get_name()
                if name in translated_map:
                    chapter_idx += 1
                    translated_content = translated_map[name]
                    display_title, body = self._extract_chapter_title(translated_content, fallback_index=chapter_idx)

                    # å°è¯•åœ¨åŸå§‹ HTML ç»“æ„ä¸­æ›¿æ¢æ–‡æœ¬
                    raw = item.get_content()
                    html_str = raw.decode('utf-8', errors='replace') if isinstance(raw, bytes) else str(raw)
                    _, segments = self.parse_html_structured(html_str)

                    if segments:
                        # ç»“æ„ä¿ç•™æ¨¡å¼ï¼šå°†ç¿»è¯‘æ–‡æœ¬å›æ³¨åˆ°åŸå§‹ HTML ç»“æ„
                        translated_body_html = self.rebuild_chapter_html(segments, translated_content)
                    else:
                        # æ— æ³•è§£æç»“æ„ï¼Œå›é€€åˆ°ç®€å•åŒ…è£…
                        translated_body_html = self._text_to_html_paragraphs(body)

                    # ä»åŸå§‹ HTML æå– <head> éƒ¨åˆ†ï¼ˆä¿ç•™ CSS é“¾æ¥ï¼‰
                    orig_soup = BeautifulSoup(html_str, "html.parser")
                    head_tag = orig_soup.find("head")
                    if head_tag:
                        head_html = str(head_tag)
                    else:
                        safe_title = display_title.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
                        head_html = f"<head><title>{safe_title}</title></head>"

                    full_html = (
                        f'<?xml version="1.0" encoding="utf-8"?>\n'
                        f'<!DOCTYPE html>\n'
                        f'<html xmlns="http://www.w3.org/1999/xhtml" lang="zh">\n'
                        f'{head_html}\n'
                        f'<body>\n{translated_body_html}\n</body>\n</html>'
                    )

                    ch = epub.EpubHtml(
                        title=display_title,
                        file_name=name,  # ä¿ç•™åŸå§‹æ–‡ä»¶å
                        lang="zh",
                    )
                    ch.set_content(full_html.encode("utf-8"))
                    book.add_item(ch)
                    spine.append(ch)
                    toc.append(ch)
                # è·³è¿‡æœªç¿»è¯‘çš„ç« èŠ‚ï¼ˆå¦‚å°é¢ã€ç›®å½•ç­‰ï¼‰
        else:
            # æ— åŸå§‹æ–‡ä»¶ï¼Œç®€å•æ„å»ºæ¨¡å¼
            for i, (filename, content) in enumerate(sorted_data):
                display_title, body = self._extract_chapter_title(content, fallback_index=i + 1)
                html_body = self._text_to_html_paragraphs(body)
                safe_title = display_title.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")

                ch = epub.EpubHtml(
                    title=display_title,
                    file_name=f"chapter_{i+1:04d}.xhtml",
                    lang="zh",
                )
                html_str = (
                    f'<?xml version="1.0" encoding="utf-8"?>\n'
                    f"<!DOCTYPE html>\n"
                    f'<html xmlns="http://www.w3.org/1999/xhtml" lang="zh">\n'
                    f"<head><title>{safe_title}</title>\n"
                    f'<link rel="stylesheet" href="style/default.css" type="text/css"/>\n'
                    f"</head>\n"
                    f"<body>\n<h2>{safe_title}</h2>\n{html_body}\n</body>\n</html>"
                )
                ch.set_content(html_str.encode("utf-8"))
                book.add_item(ch)
                spine.append(ch)
                toc.append(ch)

            # æ·»åŠ é»˜è®¤æ ·å¼
            style = epub.EpubItem(
                uid="style",
                file_name="style/default.css",
                media_type="text/css",
                content=(
                    b"body{font-family:serif;line-height:1.8;padding:1em;} "
                    b"p{text-indent:2em;margin:0.5em 0;} "
                    b"h2{text-align:center;margin:1em 0;}"
                ),
            )
            book.add_item(style)

        book.toc = toc
        book.spine = spine
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        epub.write_epub(output_path, book)

    @staticmethod
    def _text_to_html_paragraphs(text: str) -> str:
        """å°†çº¯æ–‡æœ¬è½¬æ¢ä¸º HTML æ®µè½"""
        paragraphs = text.split("\n")
        html_parts = []
        for p in paragraphs:
            p = p.strip()
            if p:
                p = p.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;")
                html_parts.append(f"<p>{p}</p>")
        return "\n".join(html_parts)

    # â”€â”€ ç¿»è¯‘ä¸»æµç¨‹ â”€â”€

    def start_translation(self):
        thread = threading.Thread(target=self._run_translation, daemon=True)
        thread.start()
        return thread

    def _run_translation(self):
        try:
            self.progress = TranslationProgress()
            self.progress.is_running = True
            self.progress.start_time = time.time()

            self._init_provider()
            self.glossary = self.load_glossary()
            self.system_prompt = self.build_system_prompt()

            self.log(f"ğŸ“– æ­£åœ¨è¯»å–: {os.path.basename(self.config.input_file)}")
            chapters = self.get_chapters()
            self.log(f"ğŸ“š å…± {len(chapters)} ä¸ªæœ‰æ•ˆç« èŠ‚")

            start = max(0, self.config.start_chapter - 1) if self.config.start_chapter > 0 else 0
            end = (
                self.config.end_chapter
                if 0 < self.config.end_chapter <= len(chapters)
                else len(chapters)
            )
            target_chapters = chapters[start:end]
            self.progress.total_chapters = len(target_chapters)
            self.log(f"ğŸ¯ èŒƒå›´: ç¬¬ {start+1} ~ {end} ç«  (å…± {len(target_chapters)} ç« )")
            self.log(f"ğŸ“„ è¾“å‡ºæ ¼å¼: {self.config.output_format.upper()}")

            if self.config.chunk_size <= 0:
                self.log("ğŸ“‹ æ•´ç« ç¿»è¯‘æ¨¡å¼: æ¯ç« ä½œä¸ºä¸€ä¸ªæ•´ä½“å‘é€")
            else:
                self.log(f"ğŸ“‹ åˆ†å—å¤§å°: {self.config.chunk_size} å­—")
            if self.config.context_lines > 0:
                self.log(f"ğŸ”— ä¸Šä¸‹æ–‡æ³¨å…¥: å‰æ–‡ {self.config.context_lines} è¡Œ")
            if self.config.concurrent_workers > 1:
                self.log(f"âš¡ å¹¶å‘: {self.config.concurrent_workers} çº¿ç¨‹")

            if self.config.enable_checkpoint:
                self.checkpoint = CheckpointManager(self.config.input_file, self.config.output_file)
                self.checkpoint.load()
                done = self.checkpoint.get_completed_count()
                if done > 0:
                    self.log(f"ğŸ“Œ æ–­ç‚¹ç»­ä¼ : å·²å®Œæˆ {done} ç« ï¼Œè‡ªåŠ¨è·³è¿‡")

            output_dir = os.path.dirname(self.config.output_file)
            if output_dir and not os.path.exists(output_dir):
                os.makedirs(output_dir, exist_ok=True)

            chapters_data = []

            for i, chapter in enumerate(target_chapters):
                if self.progress.is_cancelled:
                    self.log("âŒ ç¿»è¯‘å·²å–æ¶ˆ")
                    break

                self._pause_event.wait()

                self.progress.current_chapter = i + 1
                self.progress.current_chapter_name = chapter.name
                self.progress.current_chunk = 0

                if (
                    self.config.enable_checkpoint
                    and self.checkpoint
                    and self.checkpoint.is_chapter_done(chapter.name)
                ):
                    cached = self.checkpoint.get_chapter_result(chapter.name)
                    chapters_data.append((chapter.name, cached))
                    self.log(f"â© [{i+1}/{len(target_chapters)}] {chapter.name} (å·²ç¼“å­˜)")
                    self.progress.translated_chars += len(cached)
                    self.progress.elapsed_time = time.time() - self.progress.start_time
                    if self.on_progress:
                        self.on_progress(self.progress)
                    continue

                if self.on_chapter_start:
                    self.on_chapter_start(chapter)
                self.log(f"ğŸ“ [{i+1}/{len(target_chapters)}] {chapter.name}")

                chunks = self.split_text(chapter.content)
                self.progress.total_chunks = len(chunks)
                translated_parts = self._translate_chunks(chunks)
                translated_content = "\n".join(translated_parts)
                chapters_data.append((chapter.name, translated_content))

                if self.config.enable_checkpoint and self.checkpoint:
                    self.checkpoint.mark_chapter_done(chapter.name, translated_content)

                self.progress.elapsed_time = time.time() - self.progress.start_time
                if self.on_progress:
                    self.on_progress(self.progress)

            if not self.progress.is_cancelled and chapters_data:
                fmt = self.config.output_format.lower()
                self.log(f"ğŸ“¦ æ­£åœ¨ç”Ÿæˆ {fmt.upper()} æ–‡ä»¶ï¼ˆå…± {len(chapters_data)} ç« ï¼‰...")
                if fmt == "epub":
                    self._write_epub(self.config.output_file, chapters_data)
                else:
                    self._write_txt(self.config.output_file, chapters_data)
                self.log(f"âœ… å·²ä¿å­˜: {self.config.output_file}")

            self.progress.is_running = False
            self.progress.elapsed_time = time.time() - self.progress.start_time

            if not self.progress.is_cancelled:
                self.log(
                    f"âœ… å®Œæˆ! ç”¨æ—¶ {self.progress.elapsed_time:.1f}s, "
                    f"å…± {self.progress.translated_chars} å­—"
                )
                if self.on_complete:
                    self.on_complete(self.progress)

        except Exception as e:
            self.progress.is_running = False
            self.log(f"âŒ ç¿»è¯‘å‡ºé”™: {e}")
            import traceback
            self.log(traceback.format_exc())
            if self.on_error:
                self.on_error(str(e))

    # â”€â”€ æ§åˆ¶ â”€â”€

    def pause(self):
        self._pause_event.clear()
        self.progress.is_paused = True
        self.log("â¸ï¸ å·²æš‚åœ")

    def resume(self):
        self._pause_event.set()
        self.progress.is_paused = False
        self.log("â–¶ï¸ å·²æ¢å¤")

    def cancel(self):
        self.progress.is_cancelled = True
        self._pause_event.set()
        self.log("ğŸ›‘ æ­£åœ¨å–æ¶ˆ...")

    # â”€â”€ API æµ‹è¯• â”€â”€

    def test_api_connection(self):
        try:
            self._init_provider()
            return self.provider.test_connection()
        except Exception as e:
            return False, f"è¿æ¥å¤±è´¥: {e}"

    # â”€â”€ æ–­ç‚¹ç®¡ç† â”€â”€

    @staticmethod
    def clear_checkpoint(output_file: str, input_file: str):
        cp = CheckpointManager(input_file, output_file)
        cp.clear()

    @staticmethod
    def load_checkpoint_info(checkpoint_path: str):
        if not checkpoint_path or not os.path.exists(checkpoint_path):
            return None
        try:
            with open(checkpoint_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return data.get("completed_chapters", {}), data.get("config_hash", "")
        except Exception:
            return None

    def restore_from_checkpoint(self, checkpoint_path: str, output_path: str, output_format: str = "epub"):
        info = self.load_checkpoint_info(checkpoint_path)
        if info is None:
            self.log("âŒ æ–­ç‚¹æ–‡ä»¶åŠ è½½å¤±è´¥")
            return False
        completed, _ = info
        if not completed:
            self.log("âŒ æ–­ç‚¹æ–‡ä»¶ä¸­æ— å·²å®Œæˆç« èŠ‚")
            return False

        self.log(f"ğŸ“Œ æ–­ç‚¹æ–‡ä»¶åŒ…å« {len(completed)} ä¸ªå·²ç¿»è¯‘ç« èŠ‚")

        chapters_data = []
        if self.config.input_file and os.path.exists(self.config.input_file):
            try:
                chapters = self.get_chapters()
                missing = []
                for ch in chapters:
                    if ch.name in completed:
                        chapters_data.append((ch.name, completed[ch.name]))
                    else:
                        missing.append(ch.name)
                if missing:
                    self.log(f"âš ï¸ æœ‰ {len(missing)} ä¸ªç« èŠ‚æœªåœ¨æ–­ç‚¹ä¸­æ‰¾åˆ°ï¼Œå°†è·³è¿‡")
            except Exception as ex:
                self.log(f"âš ï¸ æ— æ³•è¯»å–æºæ–‡ä»¶ï¼ŒæŒ‰æ–‡ä»¶ååºå·æ’åºè¾“å‡º: {ex}")
                chapters_data = list(completed.items())
        else:
            chapters_data = list(completed.items())

        if not chapters_data:
            self.log("âŒ æ²¡æœ‰å¯è¾“å‡ºçš„ç« èŠ‚")
            return False

        fmt = output_format.lower()
        self.log(f"ğŸ“¦ æ­£åœ¨ç”Ÿæˆ {fmt.upper()} æ–‡ä»¶ï¼ˆå…± {len(chapters_data)} ç« ï¼‰: {output_path}")
        if fmt == "epub":
            self._write_epub(output_path, chapters_data)
        else:
            self._write_txt(output_path, chapters_data)
        self.log(f"âœ… å·²ä¿å­˜: {output_path} ({os.path.getsize(output_path)} bytes)")
        return True

    # ============== ç¿»è¯‘ä¿®å¤ (Quality Scan & Retranslation) ==============

    def quality_scan(self, checkpoint_path: str, rules: dict | None = None) -> dict:
        """æ‰«ææ–­ç‚¹æ–‡ä»¶ä¸­çš„ç¿»è¯‘è´¨é‡é—®é¢˜ã€‚

        Args:
            checkpoint_path: æ–­ç‚¹æ–‡ä»¶è·¯å¾„
            rules: {å…³é”®è¯: è¯´æ˜}ï¼Œä¸º None æˆ–ç©ºåˆ™ä¸æ£€æŸ¥

        Returns:
            {chapter_name: [(å…³é”®è¯, å‡ºç°æ¬¡æ•°, è¯´æ˜), ...]}
        """
        if not rules:
            self.log("â„¹ï¸ æœªæä¾›æ£€æŸ¥è§„åˆ™ï¼Œè¯·åœ¨è¾“å…¥æ¡†ä¸­å¡«å†™è¦æ£€æŸ¥çš„å…³é”®è¯")
            return {}

        info = self.load_checkpoint_info(checkpoint_path)
        if info is None:
            self.log("âŒ æ— æ³•åŠ è½½æ–­ç‚¹æ–‡ä»¶è¿›è¡Œè´¨é‡æ‰«æ")
            return {}

        completed, _ = info
        issues = {}
        for ch_name, text in completed.items():
            ch_issues = []
            for keyword, hint in rules.items():
                count = text.count(keyword)
                if count > 0:
                    ch_issues.append((keyword, count, hint))
            if ch_issues:
                issues[ch_name] = ch_issues
        return issues

    def retranslate_chapters(
        self,
        checkpoint_path: str,
        chapter_names: list[str],
        output_path: str | None = None,
        output_format: str = "epub",
        on_retranslate_progress=None,
    ) -> bool:
        """é€‰æ‹©æ€§é‡ç¿»æŒ‡å®šç« èŠ‚å¹¶æ›´æ–°æ–­ç‚¹"""
        if not checkpoint_path or not os.path.exists(checkpoint_path):
            self.log("âŒ æ–­ç‚¹æ–‡ä»¶ä¸å­˜åœ¨")
            return False

        try:
            with open(checkpoint_path, "r", encoding="utf-8") as f:
                cp_data = json.load(f)
        except Exception as e:
            self.log(f"âŒ åŠ è½½æ–­ç‚¹å¤±è´¥: {e}")
            return False

        completed = cp_data.get("completed_chapters", {})
        if not completed:
            self.log("âŒ æ–­ç‚¹æ–‡ä»¶æ— å·²ç¿»è¯‘ç« èŠ‚")
            return False

        if not self.config.input_file or not os.path.exists(self.config.input_file):
            self.log("âŒ æº EPUB æ–‡ä»¶æœªæŒ‡å®šæˆ–ä¸å­˜åœ¨")
            return False

        all_chapters = self.get_chapters()
        name_to_chapter = {ch.name: ch for ch in all_chapters}

        valid_names = [n for n in chapter_names if n in name_to_chapter and n in completed]
        if not valid_names:
            self.log("âŒ æŒ‡å®šçš„ç« èŠ‚å‡ä¸åœ¨æ–­ç‚¹ä¸­æˆ–æºæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°")
            return False

        skipped = set(chapter_names) - set(valid_names)
        if skipped:
            self.log(f"âš ï¸ è·³è¿‡ {len(skipped)} ä¸ªæ— æ•ˆç« èŠ‚: {', '.join(skipped)}")

        self.log(f"ğŸ”„ å¼€å§‹é‡ç¿» {len(valid_names)} ä¸ªç« èŠ‚...")

        self._init_provider()
        self.glossary = self.load_glossary()
        self.system_prompt = self.build_system_prompt()
        self.progress.is_cancelled = False

        for idx, ch_name in enumerate(valid_names):
            if self.progress.is_cancelled:
                self.log("âŒ é‡ç¿»å·²å–æ¶ˆ")
                break

            chapter = name_to_chapter[ch_name]
            self.log(f"ğŸ“ [{idx+1}/{len(valid_names)}] é‡ç¿»: {ch_name}")

            if on_retranslate_progress:
                on_retranslate_progress(idx + 1, len(valid_names), ch_name)

            chunks = self.split_text(chapter.content)
            translated_parts = self._translate_chunks(chunks)
            translated_content = "\n".join(translated_parts)
            completed[ch_name] = translated_content

        cp_data["completed_chapters"] = completed
        try:
            with open(checkpoint_path, "w", encoding="utf-8") as f:
                json.dump(cp_data, f, ensure_ascii=False, indent=2)
            self.log(f"ğŸ’¾ æ–­ç‚¹å·²æ›´æ–°: {checkpoint_path}")
        except Exception as e:
            self.log(f"âŒ ä¿å­˜æ–­ç‚¹å¤±è´¥: {e}")
            return False

        if output_path:
            chapters_data = list(completed.items())
            fmt = output_format.lower()
            self.log(f"ğŸ“¦ æ­£åœ¨ç”Ÿæˆ {fmt.upper()} æ–‡ä»¶ï¼ˆå…± {len(chapters_data)} ç« ï¼‰: {output_path}")
            if fmt == "epub":
                self._write_epub(output_path, chapters_data)
            else:
                self._write_txt(output_path, chapters_data)
            self.log(f"âœ… å·²ä¿å­˜: {output_path} ({os.path.getsize(output_path)} bytes)")

        self.log(f"âœ… é‡ç¿»å®Œæˆ! å…± {len(valid_names)} ç« ")
        return True
